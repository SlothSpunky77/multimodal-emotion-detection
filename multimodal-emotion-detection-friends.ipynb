{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88291,"databundleVersionId":10117909,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:32:25.077044Z","iopub.execute_input":"2024-11-12T12:32:25.077598Z","iopub.status.idle":"2024-11-12T12:32:26.173342Z","shell.execute_reply.started":"2024-11-12T12:32:25.077532Z","shell.execute_reply":"2024-11-12T12:32:26.172347Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport cv2\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom tensorflow.keras.preprocessing import image\nfrom transformers import BertModel, BertTokenizer\nfrom torchvision import models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:32:38.128016Z","iopub.execute_input":"2024-11-12T12:32:38.128509Z","iopub.status.idle":"2024-11-12T12:32:55.809221Z","shell.execute_reply.started":"2024-11-12T12:32:38.128471Z","shell.execute_reply":"2024-11-12T12:32:55.808294Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"nltk.download('wordnet')\nnltk.download('omw-1.4')  # Also download the Open Multilingual Wordnet for better support\nnltk.download('stopwords')  # Ensure stopwords are also downloaded\nnltk.download('punkt')  # If you haven't downloaded the tokenizer resources yet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:32:59.715715Z","iopub.execute_input":"2024-11-12T12:32:59.716644Z","iopub.status.idle":"2024-11-12T12:33:00.129633Z","shell.execute_reply.started":"2024-11-12T12:32:59.716588Z","shell.execute_reply":"2024-11-12T12:33:00.128744Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/train.csv')\n# Define path to video clips\nvideo_dir = '/kaggle/input/PES-ml-hack-link2/train_videos'\n\n\n# Function to get video file path from IDs\ndef get_video_clip_path(row):\n    dialogue_id = row['Dialogue_ID']\n    utterance_id = row['Utterance_ID']\n    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n    return os.path.join(video_dir, filename)\n\n# Apply the function to get file paths for each sampled clip\ntrain_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)\n\n# Check sample paths\nprint(train_df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:33:06.867337Z","iopub.execute_input":"2024-11-12T12:33:06.868024Z","iopub.status.idle":"2024-11-12T12:33:06.921395Z","shell.execute_reply.started":"2024-11-12T12:33:06.867972Z","shell.execute_reply":"2024-11-12T12:33:06.920457Z"}},"outputs":[{"name":"stdout","text":"   Dialogue_ID  Utterance_ID  \\\n0          231             3   \n1          549             1   \n2          432             0   \n3          774             3   \n4          849             3   \n\n                                     video_clip_path  \n0  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n1  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n2  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n3  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n4  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Define path to video clips\ndf = pd.read_csv('/kaggle/input/PES-ml-hack-link1/test.csv', encoding='unicode_escape')\nvideo_dir = '/kaggle/input/PES-ml-hack-link2/test_videos'\n\n\n# Function to get video file path from IDs\ndef get_video_clip_path(row):\n    dialogue_id = row['Dialogue_ID']\n    utterance_id = row['Utterance_ID']\n    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n    return os.path.join(video_dir, filename)\n\n# Apply the function to get file paths for each sampled clip\ndf['video_clip_path'] = df.apply(get_video_clip_path, axis=1)\n\n# Check sample paths\nprint(df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:33:11.199404Z","iopub.execute_input":"2024-11-12T12:33:11.199775Z","iopub.status.idle":"2024-11-12T12:33:11.227234Z","shell.execute_reply.started":"2024-11-12T12:33:11.199737Z","shell.execute_reply":"2024-11-12T12:33:11.226240Z"}},"outputs":[{"name":"stdout","text":"   Dialogue_ID  Utterance_ID  \\\n0            9             6   \n1           12             3   \n2           16             0   \n3           22             3   \n4           23             2   \n\n                                     video_clip_path  \n0  /kaggle/input/PES-ml-hack-link2/test_videos/di...  \n1  /kaggle/input/PES-ml-hack-link2/test_videos/di...  \n2  /kaggle/input/PES-ml-hack-link2/test_videos/di...  \n3  /kaggle/input/PES-ml-hack-link2/test_videos/di...  \n4  /kaggle/input/PES-ml-hack-link2/test_videos/di...  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:33:15.744806Z","iopub.execute_input":"2024-11-12T12:33:15.745558Z","iopub.status.idle":"2024-11-12T12:33:15.760485Z","shell.execute_reply.started":"2024-11-12T12:33:15.745520Z","shell.execute_reply":"2024-11-12T12:33:15.759479Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Sr No.                                          Utterance       Speaker  \\\n0    2328                                         Im sorry.          Ross   \n1    5461  Were actually at the end of one of our resear...  Receptionist   \n2    4231                                   Is she in there?        Phoebe   \n3    7707  This is a safe street, this is a safe building...        Rachel   \n4    8499        (noticing a kid who has picked up a copy of         Carol   \n\n   Emotion  Dialogue_ID  Utterance_ID  Season  Episode     StartTime  \\\n0  neutral          231             3       3       15   0:10:23,837   \n1  neutral          549             1       6       17  00:07:08,845   \n2  neutral          432             0       2       24  00:17:12,936   \n3  neutral          774             3       2        4  00:00:14,055   \n4  neutral          849             3       5       18  00:04:04,452   \n\n        EndTime                                    video_clip_path  \n0   0:10:24,589  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n1  00:07:13,473  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n2  00:17:14,188  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n3  00:00:17,391  /kaggle/input/PES-ml-hack-link2/train_videos/d...  \n4  00:04:06,453  /kaggle/input/PES-ml-hack-link2/train_videos/d...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sr No.</th>\n      <th>Utterance</th>\n      <th>Speaker</th>\n      <th>Emotion</th>\n      <th>Dialogue_ID</th>\n      <th>Utterance_ID</th>\n      <th>Season</th>\n      <th>Episode</th>\n      <th>StartTime</th>\n      <th>EndTime</th>\n      <th>video_clip_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2328</td>\n      <td>Im sorry.</td>\n      <td>Ross</td>\n      <td>neutral</td>\n      <td>231</td>\n      <td>3</td>\n      <td>3</td>\n      <td>15</td>\n      <td>0:10:23,837</td>\n      <td>0:10:24,589</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5461</td>\n      <td>Were actually at the end of one of our resear...</td>\n      <td>Receptionist</td>\n      <td>neutral</td>\n      <td>549</td>\n      <td>1</td>\n      <td>6</td>\n      <td>17</td>\n      <td>00:07:08,845</td>\n      <td>00:07:13,473</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4231</td>\n      <td>Is she in there?</td>\n      <td>Phoebe</td>\n      <td>neutral</td>\n      <td>432</td>\n      <td>0</td>\n      <td>2</td>\n      <td>24</td>\n      <td>00:17:12,936</td>\n      <td>00:17:14,188</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7707</td>\n      <td>This is a safe street, this is a safe building...</td>\n      <td>Rachel</td>\n      <td>neutral</td>\n      <td>774</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>00:00:14,055</td>\n      <td>00:00:17,391</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8499</td>\n      <td>(noticing a kid who has picked up a copy of</td>\n      <td>Carol</td>\n      <td>neutral</td>\n      <td>849</td>\n      <td>3</td>\n      <td>5</td>\n      <td>18</td>\n      <td>00:04:04,452</td>\n      <td>00:04:06,453</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:33:16.727429Z","iopub.execute_input":"2024-11-12T12:33:16.728257Z","iopub.status.idle":"2024-11-12T12:33:16.737639Z","shell.execute_reply.started":"2024-11-12T12:33:16.728218Z","shell.execute_reply":"2024-11-12T12:33:16.736680Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Sr No.             0\nUtterance          0\nSpeaker            0\nEmotion            0\nDialogue_ID        0\nUtterance_ID       0\nSeason             0\nEpisode            0\nStartTime          0\nEndTime            0\nvideo_clip_path    0\ndtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# No null values are present. Checking summary statistic of mumerical columns\ntrain_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:33:17.679052Z","iopub.execute_input":"2024-11-12T12:33:17.679459Z","iopub.status.idle":"2024-11-12T12:33:17.706750Z","shell.execute_reply.started":"2024-11-12T12:33:17.679423Z","shell.execute_reply":"2024-11-12T12:33:17.705794Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"             Sr No.  Dialogue_ID  Utterance_ID      Season     Episode\ncount    999.000000   999.000000    999.000000  999.000000  999.000000\nmean    5324.614615   532.546547      5.918919    4.950951   12.816817\nstd     3057.803271   305.094456      4.911688    2.341701    7.262021\nmin       23.000000     1.000000      0.000000    1.000000    1.000000\n25%     2598.500000   256.000000      2.000000    3.000000    6.000000\n50%     5461.000000   549.000000      5.000000    5.000000   13.000000\n75%     7972.000000   804.000000      9.000000    7.000000   19.000000\nmax    10474.000000  1038.000000     21.000000    9.000000   25.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sr No.</th>\n      <th>Dialogue_ID</th>\n      <th>Utterance_ID</th>\n      <th>Season</th>\n      <th>Episode</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>999.000000</td>\n      <td>999.000000</td>\n      <td>999.000000</td>\n      <td>999.000000</td>\n      <td>999.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5324.614615</td>\n      <td>532.546547</td>\n      <td>5.918919</td>\n      <td>4.950951</td>\n      <td>12.816817</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3057.803271</td>\n      <td>305.094456</td>\n      <td>4.911688</td>\n      <td>2.341701</td>\n      <td>7.262021</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>23.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2598.500000</td>\n      <td>256.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5461.000000</td>\n      <td>549.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>13.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7972.000000</td>\n      <td>804.000000</td>\n      <td>9.000000</td>\n      <td>7.000000</td>\n      <td>19.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10474.000000</td>\n      <td>1038.000000</td>\n      <td>21.000000</td>\n      <td>9.000000</td>\n      <td>25.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:33:18.466492Z","iopub.execute_input":"2024-11-12T12:33:18.466858Z","iopub.status.idle":"2024-11-12T12:33:18.473608Z","shell.execute_reply.started":"2024-11-12T12:33:18.466823Z","shell.execute_reply":"2024-11-12T12:33:18.472493Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Index(['Sr No.', 'Utterance', 'Speaker', 'Emotion', 'Dialogue_ID',\n       'Utterance_ID', 'Season', 'Episode', 'StartTime', 'EndTime',\n       'video_clip_path'],\n      dtype='object')"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:33:23.088898Z","iopub.execute_input":"2024-11-12T12:33:23.089316Z","iopub.status.idle":"2024-11-12T12:33:24.403589Z","shell.execute_reply.started":"2024-11-12T12:33:23.089279Z","shell.execute_reply":"2024-11-12T12:33:24.402644Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n# Function for text preprocessing\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Tokenization\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Lemmatization\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Join tokens back into a string\n    return ' '.join(tokens)\n\n# Apply preprocessing to 'Utterance' column\ntrain_df['temp_Utterance'] = train_df['Utterance'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:33:24.405577Z","iopub.execute_input":"2024-11-12T12:33:24.405917Z","iopub.status.idle":"2024-11-12T12:33:26.897970Z","shell.execute_reply.started":"2024-11-12T12:33:24.405883Z","shell.execute_reply":"2024-11-12T12:33:26.897202Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Do the same pre processing for test dataset\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n# Function for text preprocessing\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Tokenization\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Lemmatization\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    # Join tokens back into a string\n    return ' '.join(tokens)\n\n# Apply preprocessing to 'Utterance' column\ndf['temp_Utterance'] = df['Utterance'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:34:21.277446Z","iopub.execute_input":"2024-11-12T12:34:21.278206Z","iopub.status.idle":"2024-11-12T12:34:21.304392Z","shell.execute_reply.started":"2024-11-12T12:34:21.278165Z","shell.execute_reply":"2024-11-12T12:34:21.303662Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:34:25.932677Z","iopub.execute_input":"2024-11-12T12:34:25.933422Z","iopub.status.idle":"2024-11-12T12:34:25.948655Z","shell.execute_reply.started":"2024-11-12T12:34:25.933382Z","shell.execute_reply":"2024-11-12T12:34:25.947650Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   Sr No.                              Utterance     Speaker  Dialogue_ID  \\\n0      88                             Excuse me?  Fireman #1            9   \n1     114       I mean, enough of the silliness!        Ross           12   \n2     159  Hey, Joey, could you pass the cheese?      Monica           16   \n3     230                          Yeah, it did.      Monica           22   \n4     251               What? YetiI mean Danny?      Rachel           23   \n\n   Utterance_ID  Season  Episode     StartTime       EndTime  \\\n0             6       6       18  00:05:18,443  00:05:21,361   \n1             3       6        3   0:12:55,691   0:12:58,073   \n2             0       4       10  00:11:36,362  00:11:38,405   \n3             3       5        5  00:18:31,902  00:18:33,111   \n4             2       5        6  00:13:36,941  00:13:42,904   \n\n                                     video_clip_path  \\\n0  /kaggle/input/PES-ml-hack-link2/test_videos/di...   \n1  /kaggle/input/PES-ml-hack-link2/test_videos/di...   \n2  /kaggle/input/PES-ml-hack-link2/test_videos/di...   \n3  /kaggle/input/PES-ml-hack-link2/test_videos/di...   \n4  /kaggle/input/PES-ml-hack-link2/test_videos/di...   \n\n              temp_Utterance  \n0                     excuse  \n1      mean enough silliness  \n2  hey joey could pas cheese  \n3                       yeah  \n4           yetii mean danny  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sr No.</th>\n      <th>Utterance</th>\n      <th>Speaker</th>\n      <th>Dialogue_ID</th>\n      <th>Utterance_ID</th>\n      <th>Season</th>\n      <th>Episode</th>\n      <th>StartTime</th>\n      <th>EndTime</th>\n      <th>video_clip_path</th>\n      <th>temp_Utterance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>88</td>\n      <td>Excuse me?</td>\n      <td>Fireman #1</td>\n      <td>9</td>\n      <td>6</td>\n      <td>6</td>\n      <td>18</td>\n      <td>00:05:18,443</td>\n      <td>00:05:21,361</td>\n      <td>/kaggle/input/PES-ml-hack-link2/test_videos/di...</td>\n      <td>excuse</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>114</td>\n      <td>I mean, enough of the silliness!</td>\n      <td>Ross</td>\n      <td>12</td>\n      <td>3</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0:12:55,691</td>\n      <td>0:12:58,073</td>\n      <td>/kaggle/input/PES-ml-hack-link2/test_videos/di...</td>\n      <td>mean enough silliness</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>159</td>\n      <td>Hey, Joey, could you pass the cheese?</td>\n      <td>Monica</td>\n      <td>16</td>\n      <td>0</td>\n      <td>4</td>\n      <td>10</td>\n      <td>00:11:36,362</td>\n      <td>00:11:38,405</td>\n      <td>/kaggle/input/PES-ml-hack-link2/test_videos/di...</td>\n      <td>hey joey could pas cheese</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>230</td>\n      <td>Yeah, it did.</td>\n      <td>Monica</td>\n      <td>22</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>00:18:31,902</td>\n      <td>00:18:33,111</td>\n      <td>/kaggle/input/PES-ml-hack-link2/test_videos/di...</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>251</td>\n      <td>What? YetiI mean Danny?</td>\n      <td>Rachel</td>\n      <td>23</td>\n      <td>2</td>\n      <td>5</td>\n      <td>6</td>\n      <td>00:13:36,941</td>\n      <td>00:13:42,904</td>\n      <td>/kaggle/input/PES-ml-hack-link2/test_videos/di...</td>\n      <td>yetii mean danny</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:34:28.505163Z","iopub.execute_input":"2024-11-12T12:34:28.506533Z","iopub.status.idle":"2024-11-12T12:34:28.521068Z","shell.execute_reply.started":"2024-11-12T12:34:28.506474Z","shell.execute_reply":"2024-11-12T12:34:28.519955Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   Sr No.                                          Utterance       Speaker  \\\n0    2328                                         Im sorry.          Ross   \n1    5461  Were actually at the end of one of our resear...  Receptionist   \n2    4231                                   Is she in there?        Phoebe   \n3    7707  This is a safe street, this is a safe building...        Rachel   \n4    8499        (noticing a kid who has picked up a copy of         Carol   \n\n   Emotion  Dialogue_ID  Utterance_ID  Season  Episode     StartTime  \\\n0  neutral          231             3       3       15   0:10:23,837   \n1  neutral          549             1       6       17  00:07:08,845   \n2  neutral          432             0       2       24  00:17:12,936   \n3  neutral          774             3       2        4  00:00:14,055   \n4  neutral          849             3       5       18  00:04:04,452   \n\n        EndTime                                    video_clip_path  \\\n0   0:10:24,589  /kaggle/input/PES-ml-hack-link2/train_videos/d...   \n1  00:07:13,473  /kaggle/input/PES-ml-hack-link2/train_videos/d...   \n2  00:17:14,188  /kaggle/input/PES-ml-hack-link2/train_videos/d...   \n3  00:00:17,391  /kaggle/input/PES-ml-hack-link2/train_videos/d...   \n4  00:04:06,453  /kaggle/input/PES-ml-hack-link2/train_videos/d...   \n\n                                      temp_Utterance  \n0                                           im sorry  \n1  actually end one research cycle looking applic...  \n2                                                     \n3            safe street safe building there nothing  \n4                           noticing kid picked copy  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sr No.</th>\n      <th>Utterance</th>\n      <th>Speaker</th>\n      <th>Emotion</th>\n      <th>Dialogue_ID</th>\n      <th>Utterance_ID</th>\n      <th>Season</th>\n      <th>Episode</th>\n      <th>StartTime</th>\n      <th>EndTime</th>\n      <th>video_clip_path</th>\n      <th>temp_Utterance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2328</td>\n      <td>Im sorry.</td>\n      <td>Ross</td>\n      <td>neutral</td>\n      <td>231</td>\n      <td>3</td>\n      <td>3</td>\n      <td>15</td>\n      <td>0:10:23,837</td>\n      <td>0:10:24,589</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n      <td>im sorry</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5461</td>\n      <td>Were actually at the end of one of our resear...</td>\n      <td>Receptionist</td>\n      <td>neutral</td>\n      <td>549</td>\n      <td>1</td>\n      <td>6</td>\n      <td>17</td>\n      <td>00:07:08,845</td>\n      <td>00:07:13,473</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n      <td>actually end one research cycle looking applic...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4231</td>\n      <td>Is she in there?</td>\n      <td>Phoebe</td>\n      <td>neutral</td>\n      <td>432</td>\n      <td>0</td>\n      <td>2</td>\n      <td>24</td>\n      <td>00:17:12,936</td>\n      <td>00:17:14,188</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7707</td>\n      <td>This is a safe street, this is a safe building...</td>\n      <td>Rachel</td>\n      <td>neutral</td>\n      <td>774</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>00:00:14,055</td>\n      <td>00:00:17,391</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n      <td>safe street safe building there nothing</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8499</td>\n      <td>(noticing a kid who has picked up a copy of</td>\n      <td>Carol</td>\n      <td>neutral</td>\n      <td>849</td>\n      <td>3</td>\n      <td>5</td>\n      <td>18</td>\n      <td>00:04:04,452</td>\n      <td>00:04:06,453</td>\n      <td>/kaggle/input/PES-ml-hack-link2/train_videos/d...</td>\n      <td>noticing kid picked copy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"embedding_dim = 128\ntext_hidden_dim = 128\nvideo_feature_dim = 128\nnum_classes = train_df['Emotion'].nunique()\nmax_text_length = 50\nframe_size = 224\nframes_per_video = 16  # Number of frames sampled per video\nbatch_size = 16\nnum_epochs = 10\nlearning_rate = 0.001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T12:34:34.259218Z","iopub.execute_input":"2024-11-12T12:34:34.259933Z","iopub.status.idle":"2024-11-12T12:34:34.268251Z","shell.execute_reply.started":"2024-11-12T12:34:34.259895Z","shell.execute_reply":"2024-11-12T12:34:34.267348Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Encode labels\nlabel_encoder = LabelEncoder()\ntrain_df['Emotion'] = label_encoder.fit_transform(train_df['Emotion'])\n\n# Text Preprocessing\ndef preprocess_text(text, vocab, max_len):\n    tokens = text.lower().split()\n    encoded = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n    return encoded[:max_len] + [0] * (max_len - len(encoded))\n\n# Build vocabulary from text data\nvocab = {'<PAD>': 0, '<UNK>': 1}\nfor utterance in train_df['temp_Utterance']:\n    for word in utterance.lower().split():\n        if word not in vocab:\n            vocab[word] = len(vocab)\n\n# Video Preprocessing \n\ndef load_video_frames(video_path, frame_size, frames_per_video):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    sample_interval = max(total_frames // frames_per_video, 1)\n\n    count = 0\n    while len(frames) < frames_per_video:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if count % sample_interval == 0:\n            frame = cv2.resize(frame, (frame_size, frame_size))\n            frame = torch.tensor(frame).permute(2, 0, 1) / 255.0  # Normalize to [0, 1]\n            frames.append(frame)\n        count += 1\n\n    cap.release()\n\n    # Ensure each video has exactly frames_per_video frames by padding or truncating\n    if len(frames) < frames_per_video:\n        padding_frames = [torch.zeros(3, frame_size, frame_size) for _ in range(frames_per_video - len(frames))]\n        frames.extend(padding_frames)  # Pad with empty frames if not enough frames\n        if len(frames) != frames_per_video:\n           print(f\"Padding issue Video: {video_path}, Frames after padding: {len(frames)}\")\n    elif len(frames) > frames_per_video:\n        frames = frames[:frames_per_video]  # Truncate if too many frames\n\n    #print(f\"Loaded frames for video {video_path}: {len(frames)}\")\n    if len(frames) != frames_per_video:\n        print(f\"Mismatch detected! Expected {frames_per_video} frames, but got {len(frames)}.\")\n    return torch.stack(frames)\n\n\n# Custom Dataset Class\nclass EmotionDataset(data.Dataset):\n    def __init__(self, df, vocab, max_text_len, frame_size, frames_per_video):\n        self.df = df\n        self.vocab = vocab\n        self.max_text_len = max_text_len\n        self.frame_size = frame_size\n        self.frames_per_video = frames_per_video\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        text = preprocess_text(self.df.iloc[idx]['temp_Utterance'], self.vocab, self.max_text_len)\n        video_path = self.df.iloc[idx]['video_clip_path']\n        label = self.df.iloc[idx]['Emotion']\n        frames = load_video_frames(video_path, self.frame_size, self.frames_per_video)\n        #print(f\"Text shape: {len(text)}, Frames shape: {frames.shape}, Label: {label}\")\n        if frames.shape[0] != frames_per_video or frames.shape[1:] != (3, frame_size, frame_size):\n            print(f\"Frame mismatch for video {self.df.iloc[idx]['video_clip_path']}!\")\n        return torch.tensor(text), frames, torch.tensor(label)\n\n# Data Loader\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['Emotion'])\ntrain_dataset = EmotionDataset(train_df, vocab, max_text_length, frame_size, frames_per_video)\nval_dataset = EmotionDataset(val_df, vocab, max_text_length, frame_size, frames_per_video)\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:04:10.126152Z","iopub.execute_input":"2024-11-12T13:04:10.126602Z","iopub.status.idle":"2024-11-12T13:04:10.156792Z","shell.execute_reply.started":"2024-11-12T13:04:10.126547Z","shell.execute_reply":"2024-11-12T13:04:10.155859Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(TextEncoder, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.fc = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n    \n    def forward(self, text_input):\n        outputs = self.bert(text_input)\n        hidden_states = outputs.last_hidden_state\n        cls_embedding = hidden_states[:, 0, :]  # [batch_size, hidden_size]\n        return self.fc(cls_embedding)  # Output hidden_dim size\n\nclass VideoEncoder(nn.Module):\n    def __init__(self, frame_size, frames_per_video):\n        super(VideoEncoder, self).__init__()\n        # Load a pre-trained ResNet model\n        self.resnet = models.resnet18(pretrained=True)\n        self.resnet.fc = nn.Identity() \n    \n    def forward(self, video_frames):\n        # Assuming video_frames is of shape [batch_size, num_frames, channels, height, width]\n        batch_size, num_frames, channels, height, width = video_frames.shape\n        video_frames = video_frames.view(batch_size * num_frames, channels, height, width)\n        \n        frame_features = self.resnet(video_frames)  # Get features for each frame\n        frame_features = frame_features.view(batch_size, num_frames, -1)  # [batch_size, num_frames, feature_size]\n        \n        # Average across frames to get a single feature vector for each video\n        video_features = torch.mean(frame_features, dim=1)\n        return video_features\n\nclass EmotionClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, frame_size, frames_per_video, num_classes=5):\n        super(EmotionClassifier, self).__init__()\n        # Use pre-trained TextEncoder and VideoEncoder\n        self.text_encoder = TextEncoder(vocab_size, embedding_dim, hidden_dim)\n        self.video_encoder = VideoEncoder(frame_size, frames_per_video)\n        \n        # Fully connected layer to combine both modalities (text and video)\n        self.fc = nn.Linear(hidden_dim + 512, num_classes)  # Adjust size based on combined features\n\n    def forward(self, text_input, video_frames):\n        text_features = self.text_encoder(text_input)\n        video_features = self.video_encoder(video_frames)\n        \n        # Concatenate the features from both text and video\n        combined_features = torch.cat((text_features, video_features), dim=1)\n        \n        # Pass through the final classification layer\n        return self.fc(combined_features)\n\n# Example usage\n#model = EmotionClassifier(vocab_size=30522, embedding_dim=128, hidden_dim=128, frame_size=224, frames_per_video=10, num_classes=5)\ntext_input = torch.randint(0, len(vocab), (16, 64))  # Example input for BERT [batch_size, sequence_length]\nvideo_frames = torch.randn(16, 10, 3, 224, 224)  # Example input for video [batch_size, num_frames, channels, height, width]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:04:39.006205Z","iopub.execute_input":"2024-11-12T13:04:39.006888Z","iopub.status.idle":"2024-11-12T13:04:39.274343Z","shell.execute_reply.started":"2024-11-12T13:04:39.006846Z","shell.execute_reply":"2024-11-12T13:04:39.273461Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = EmotionClassifier(len(vocab), embedding_dim = 128, hidden_dim=128, frame_size=224, frames_per_video=16, num_classes=5).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for text, frames, labels in train_loader:\n        text, frames, labels = text.to(device), frames.to(device), labels.to(device)\n        # print(f\"Batch Text Shape: {text.shape}\")\n        # print(f\"Batch Frames Shape: {frames.shape}\")  # Check frames shape\n        # print(f\"Batch Labels Shape: {labels.shape}\")\n        optimizer.zero_grad()\n        outputs = model(text, frames)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:52:36.708961Z","iopub.execute_input":"2024-11-12T13:52:36.710077Z","iopub.status.idle":"2024-11-12T13:52:36.717234Z","shell.execute_reply.started":"2024-11-12T13:52:36.710019Z","shell.execute_reply":"2024-11-12T13:52:36.716253Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 1.4766570603847504\nEpoch [2/10], Loss: 1.3289913543462752\nEpoch [3/10], Loss: 1.1960922189116479\nEpoch [4/10], Loss: 1.076482997020483\nEpoch [5/10], Loss: 0.9688346973184548\nEpoch [6/10], Loss: 0.8719512275866093\nEpoch [7/10], Loss: 0.7847560948279484\nEpoch [8/10], Loss: 0.7062804853451535\nEpoch [9/10], Loss: 0.6356524367106381\nEpoch [10/10], Loss: 0.5720872930395742\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    correct, total = 0, 0\n    for text, frames, labels in val_loader:\n        text, frames, labels = text.to(device), frames.to(device), labels.to(device)\n        outputs = model(text, frames)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Validation Accuracy: {accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:55:54.289508Z","iopub.execute_input":"2024-11-12T13:55:54.289882Z","iopub.status.idle":"2024-11-12T13:55:54.295207Z","shell.execute_reply.started":"2024-11-12T13:55:54.289846Z","shell.execute_reply":"2024-11-12T13:55:54.294206Z"}},"outputs":[{"name":"stdout","text":"Validation Accuracy: 71.27%\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the same device\nmodel.to(device)\n\n# Tokenize the text input\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the text input\nmax_length = 64  # Adjust based on your dataset or the model's max input length\ndef preprocess_text(text):\n    encoding = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_length,\n        truncation=True,\n        padding='max_length',\n        return_tensors='pt'\n    )\n    return encoding['input_ids'].squeeze(0)  # Remove the batch dimension\n\n# Assuming 'video_clip_path' is the column with paths to video clips\ndef preprocess_video(video_clip_path):\n    # Load the video clip and convert it into frames as needed\n    # For simplicity, assuming video_clip_path is a path to a .mp4 file\n    # You can use libraries like OpenCV or PyTorch Video to load the video\n    # Let's assume the output is a tensor of shape [num_frames, channels, height, width]\n    frames = torch.randn(10, 3, 224, 224)  # Example random frames\n    return frames\n\n# Preprocess the entire test set\ntext_inputs = [preprocess_text(utterance) for utterance in df['temp_Utterance']]\nvideo_inputs = [preprocess_video(path) for path in df['video_clip_path']]\n\n# Convert to tensors\ntext_inputs = torch.stack(text_inputs).to(device)  # Move to the correct device\nvideo_inputs = torch.stack(video_inputs).to(device)  # Move to the correct device\n\n# 3. Get Predictions from the Model\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    outputs = model(text_inputs, video_inputs)\n\n# Apply softmax to get probabilities (optional, for understanding)\nsoftmax = torch.nn.Softmax(dim=1)\nprobabilities = softmax(outputs)\n\n# Convert logits to predicted class labels\n_, predicted_classes = torch.max(probabilities, dim=1)\n\n# 4. Add Predictions to the DataFrame\ndf['Predicted_Emotion'] = predicted_classes.cpu().numpy()  # Move to CPU before converting to numpy for compatibility with pandas\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:53:49.277625Z","iopub.execute_input":"2024-11-12T13:53:49.278048Z","iopub.status.idle":"2024-11-12T13:53:53.222404Z","shell.execute_reply.started":"2024-11-12T13:53:49.278009Z","shell.execute_reply":"2024-11-12T13:53:53.221467Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481d70e4cd7147b09395bfc65f72e882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5133d82f76f84d9eb023d380dce19814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c8e9457b884de7adc444c6b593e944"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"predicted_emotions = label_encoder.inverse_transform(predicted_classes.cpu())  # Convert to emotion labels\n\n# Add the predicted emotions back to the DataFrame\ndf['Predicted_Emotion'] = predicted_emotions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:54:04.487704Z","iopub.execute_input":"2024-11-12T13:54:04.488089Z","iopub.status.idle":"2024-11-12T13:54:04.494397Z","shell.execute_reply.started":"2024-11-12T13:54:04.488050Z","shell.execute_reply":"2024-11-12T13:54:04.493226Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"df.to_csv('test2.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:54:11.316793Z","iopub.execute_input":"2024-11-12T13:54:11.317482Z","iopub.status.idle":"2024-11-12T13:54:11.326466Z","shell.execute_reply.started":"2024-11-12T13:54:11.317441Z","shell.execute_reply":"2024-11-12T13:54:11.325515Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Select the relevant columns and rename Predicted_Emotion to Emotion\nsubmission_df = df[['Sr No.', 'Predicted_Emotion']].rename(columns={'Predicted_Emotion': 'Emotion'})\n\n# Save to submission.csv\nsubmission_df.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T13:54:14.964721Z","iopub.execute_input":"2024-11-12T13:54:14.965361Z","iopub.status.idle":"2024-11-12T13:54:14.972935Z","shell.execute_reply.started":"2024-11-12T13:54:14.965311Z","shell.execute_reply":"2024-11-12T13:54:14.972059Z"}},"outputs":[],"execution_count":26}]}